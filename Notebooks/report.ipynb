{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural-Style-Transfer-and-Fast-Neural-Style-Transfer\n",
    "\n",
    "## Summarizing the traditional NST\n",
    "\n",
    "Neural Style Transfer (NST) is an image transformation task where a regular image (called the content image) is blended with the style of an artistic image (called the style image).\n",
    "\n",
    "In NST, **\"style\"** refers to the artistic and textural characteristics of an image that define its visual appearance, distinct from its content. **Texture** includes repetitive visual patterns, such as brushstrokes, geometric shapes, or surface details.\n",
    "\n",
    "Deep Convolutional Neural Networks have been trained on large datasets for image classification tasks. During this training, different layers of the network learn varying levels and depths of feature representations of images.\n",
    "\n",
    "The lower layers of the network capture fundamental features like edges, boundaries, and lines, representing basic visual elements at the pixel level. In contrast, the higher layers capture representations of the \"actual content\" while losing detailed pixel-level information. These representations are known as **\"Content Representation.\"**\n",
    "\n",
    "For constructing style representations, we do the following:\n",
    "for a particular layer conv layer, we get feature correlations from the feature maps in that layer. By including these feature correlation matrices from different layers of the network, we can construct overall **\"Style representation\"** of the image which captures the overall texture information instead of any global arrangement in the image.\n",
    "\n",
    "\"*The key finding of this paper is that the representations of content and style in the Convolutional Neural Network are separable. That is, we can manipulate both representations independently to produce new, perceptually meaningful images.*\"\n",
    "~ Gatys et. al.\n",
    "## Visualizing Content and Style Representations from Different Layers\n",
    "\n",
    "\n",
    "To visualize the information captured by a particular layer, say $L$, we can start with a random noise image, $X$, and iteratively update its pixel values so that its feature representation at layer $L$ closely matches (ideally, becomes equal to) the feature representation of a reference image. This reference image could be used to capture either content or style, depending on the objective.\n",
    "\n",
    "### Visualizing Content Representation:\n",
    "To do this, we forward pass the image $\\overrightarrow{X}$ till layer $L$ and get its content representation at layer $L$, $F_x^L$ . Similarly, we get the content representation of the content image $C$, $F_c^L$. We define the mean square error loss function:\n",
    "$$\n",
    "Loss_{content} = \\frac12 \\sum_{i,j} ((F_x^L)_{ij}^2 - (F_c^L)_{ij}^2)\n",
    "$$\n",
    "We get the gradient with respect to $\\overrightarrow{X}$  by backpropagation and we iteratively change $\\overrightarrow{X}$ through standard gradient descent.\n",
    "\n",
    "### Visualizing Style Representation  \n",
    "To visualize the style of an image, we extract its style representation by computing the **Gram matrix** at a given layer $L$. The Gram matrix $G_x^L$ for an image $\\overrightarrow{X}$ is defined as:  \n",
    "\n",
    "$$\n",
    "G_x^L = (F_x^L)(F_x^L)^T\n",
    "$$\n",
    "\n",
    "Similarly, we compute the Gram matrix for the style image $S$:  \n",
    "\n",
    "$$\n",
    "G_s^L = (F_s^L)(F_s^L)^T\n",
    "$$\n",
    "\n",
    "We define the **style loss function** as the mean square error between the Gram matrices:  \n",
    "\n",
    "$$\n",
    "Loss_{style} = \\frac{1}{4N^2M^2} \\sum_{i,j} \\left( (G_x^L)_{ij} - (G_s^L)_{ij} \\right)^2\n",
    "$$\n",
    "\n",
    "where $N$ is the number of feature maps at layer $L$, and $M$ is the spatial dimension of the feature maps.\n",
    "\n",
    "We compute the gradient of this loss with respect to $\\overrightarrow{X}$ using **backpropagation** and iteratively update $\\overrightarrow{X}$ through **standard gradient descent** to match the style of the reference style image.\n",
    "\n",
    "\n",
    "### CODE\n",
    "```python\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms import v2\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "```\n",
    "\n",
    "##### Modules in vgg19\n",
    "- **0**: conv1_1 - Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "- **1**: ReLU(inplace=True)\n",
    "- **2**: conv1_2 - Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "- **3**: ReLU(inplace=True)\n",
    "- **4**:pool1-MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "- **5**: conv2_1 - Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "- **6**: ReLU(inplace=True)\n",
    "- **7**: conv2_2 - Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "- **8**: ReLU(inplace=True)\n",
    "- **9**: pool2 - MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "- **10**: conv3_1 - Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "- **11**: ReLU(inplace=True)\n",
    "- **12**: conv3_2 - Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "- **13**: ReLU(inplace=True)\n",
    "- **14**: conv3_3 - Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "- **15**: ReLU(inplace=True)\n",
    "- **16**: conv3_4 - Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "- **17**: ReLU(inplace=True)\n",
    "- **18**: pool3 - MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "- **19**: conv4_1 - Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "- **20**: ReLU(inplace=True)\n",
    "- **21**: conv4_2 - Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "- **22**: ReLU(inplace=True)\n",
    "- **23**: conv4_3 - Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "- **24**: ReLU(inplace=True)\n",
    "- **25**: conv4_4 - Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "- **26**: ReLU(inplace=True)\n",
    "- **27**: pool4 - MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "- **28**: conv5_1 - Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "- **29**: ReLU(inplace=True)\n",
    "- **30**: conv5_2 - Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "- **31**: ReLU(inplace=True)\n",
    "- **32**: conv5_3 - Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "- **33**: ReLU(inplace=True)\n",
    "- **34**: conv5_4 - Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "- **35**: ReLU(inplace=True)\n",
    "- **36**: pool5 - MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "\n",
    "**\"*We used the feature space provided by the 16 convolutional and 5 pooling layers of the 19 layer VGG Network. For image synthesis we found that replacing the max-pooling operation by average pooling improves the gradient flow and one obtains slightly more appealing results, which is why the images shown were generated with average pooling.*\" ~ Gatys et. al.**\n",
    "\n",
    "```python\n",
    "class VGG19(nn.Module):\n",
    "    def __init__(self, pool_type: str):\n",
    "        super().__init__()\n",
    "        if pool_type == \"avg\":\n",
    "            self.layer_list = []\n",
    "            self.vgg19 = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features\n",
    "            for layer in self.vgg19:\n",
    "                if isinstance(layer, torch.nn.modules.pooling.MaxPool2d):\n",
    "                    self.layer_list.append(\n",
    "                        nn.AvgPool2d(\n",
    "                            kernel_size=2, stride=2, padding=0, ceil_mode=False\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    self.layer_list.append(layer)\n",
    "            self.final_model = nn.Sequential(*self.layer_list)\n",
    "        else:\n",
    "            self.final_model = models.vgg19().features\n",
    "\n",
    "        for param in self.final_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, input, layer_keys):\n",
    "        out = {}\n",
    "        last_layer_key = str(max([int(key) for key in layer_keys]))\n",
    "        for name, layer in self.final_model.named_children():\n",
    "            out[name] = layer(input)\n",
    "            input = out[name]\n",
    "            if name == last_layer_key:\n",
    "                return [out[key] for key in layer_keys]\n",
    "\n",
    "        return [out[key] for key in layer_keys]\n",
    "\n",
    "```\n",
    "**Defining the Neural Style transfer Class:**\n",
    "```python\n",
    "class NeuralStyleTransfer:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        content_image_path,\n",
    "        style_image_path,\n",
    "        content_layer_name,\n",
    "        style_layers_list,\n",
    "        style_layer_weights,\n",
    "        optimizer=\"LBFGS\",\n",
    "        alpha=1,\n",
    "        beta=1000,\n",
    "        lr=1,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        num_steps=500,\n",
    "        resize_value=256,\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.lr = lr\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self._optim = optimizer\n",
    "        self.content_layer = content_layer_name\n",
    "        self.style_layers_list = style_layers_list\n",
    "        self.style_weights = style_layer_weights\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        self.content_image = self.load_and_transform(\n",
    "            image_path=content_image_path, resize_value=resize_value\n",
    "        )\n",
    "\n",
    "        self.style_image = self.load_and_transform(\n",
    "            image_path=style_image_path, resize_value=resize_value\n",
    "        )\n",
    "\n",
    "        self.model = VGG19(pool_type=\"avg\").to(device).eval()\n",
    "```\n",
    "\n",
    "**Preprocessing Function for VGG19**\n",
    "\n",
    "```python\n",
    "def load_and_transform(self, image_path, resize_value):\n",
    "    image = Image.open(image_path)\n",
    "    transformations = v2.Compose(\n",
    "        [\n",
    "            v2.Resize(resize_value),\n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return transformations(image).unsqueeze(0).to(self.device)\n",
    "```\n",
    "\n",
    "**Static Method for generating white noise image:**\n",
    "\n",
    "```python\n",
    "@staticmethod\n",
    "def generate_white_noise(image, device):\n",
    "    batch, channels, h, w = image.shape\n",
    "    random_image = torch.randn(\n",
    "        size=[batch, channels, h, w], requires_grad=True, device=device\n",
    "    )\n",
    "    return random_image\n",
    "```\n",
    "\n",
    "**Static method for converting a output tensor back to PIL image:**\n",
    "\n",
    "```python\n",
    "@staticmethod\n",
    "def tensor_to_image(tensor):\n",
    "    tensor = tensor.clone().detach().squeeze(0).cpu()\n",
    "\n",
    "    tensor = tensor.mul(torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1))\n",
    "    tensor = tensor.add(torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1))\n",
    "    tensor = torch.clamp(tensor, 0, 1)\n",
    "\n",
    "    return v2.ToPILImage()(tensor)\n",
    "```\n",
    "\n",
    "**Static method for getting gram matrix** \n",
    "```python\n",
    "@staticmethod\n",
    "def get_gram_matrix(input):\n",
    "    batch, channel, height, width = input.shape\n",
    "    reshaped_input = input.view(batch, channel, height * width)\n",
    "    gram_matrix = torch.bmm(reshaped_input, reshaped_input.transpose(1, 2))\n",
    "    gram_matrix = torch.div(gram_matrix, height * width)\n",
    "    return gram_matrix\n",
    "```\n",
    "  \n",
    "**Function to visualize the content representation of a image at a particular layer.**\n",
    "The function optimizes a random noise image using either **L-BFGS** or **Adam** optimization.\n",
    "\n",
    "```python\n",
    "def visualize_content_rep(self):\n",
    "    print(f\"Using Device: {self.device}\")\n",
    "    random_image = self.generate_white_noise(self.content_image, self.device)\n",
    "\n",
    "    original_img_features = self.model(self.content_image, self.content_layer)[0].detach()\n",
    "\n",
    "    progress_bar = tqdm(range(self.num_steps), desc=\"Optimizing\", unit=\"step\")\n",
    "    if self._optim == \"LBFGS\":\n",
    "        optimizer = optim.LBFGS([random_image], lr=self.lr)\n",
    "        for step in progress_bar:\n",
    "            current_loss = [0.0]\n",
    "\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                random_img_features = self.model(random_image, self.content_layer)[0]\n",
    "                content_loss = torch.mean(\n",
    "                    (original_img_features - random_img_features) ** 2\n",
    "                )\n",
    "\n",
    "                content_loss.backward()\n",
    "                current_loss[0] = content_loss.item()\n",
    "                return content_loss\n",
    "\n",
    "            optimizer.step(closure)\n",
    "            progress_bar.set_postfix(loss=current_loss[0])\n",
    "\n",
    "    elif self._optim == \"Adam\":\n",
    "        optimizer = optim.Adam([random_image], lr=self.lr)\n",
    "        for step in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            random_img_features = self.model(random_image, self.content_layer)[0]\n",
    "            content_loss = torch.mean(\n",
    "                (original_img_features - random_img_features) ** 2\n",
    "            )\n",
    "            content_loss.backward()\n",
    "            optimizer.step()\n",
    "            progress_bar.set_postfix(loss=content_loss.item())\n",
    "    generated_image = self.tensor_to_image(random_image)\n",
    "    plt.imshow(generated_image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    return generated_image\n",
    "```\n",
    "**Function to visualize the Style representation of a image.**\n",
    "```python\n",
    "def visualize_style_rep(self):\n",
    "    print(f\"Using Device: {self.device}\")\n",
    "    random_image = self.generate_white_noise(self.style_image, self.device)\n",
    "\n",
    "    original_style_features = self.model(self.style_image, self.style_layers_list)\n",
    "    original_gram_matrices = []\n",
    "    for f in original_style_features:\n",
    "        gram_matrix = self.get_gram_matrix(f)\n",
    "        original_gram_matrices.append(gram_matrix)\n",
    "\n",
    "    progress_bar = tqdm(range(self.num_steps), desc=\"Optimizing\", unit=\"step\")\n",
    "    if self._optim == \"LBFGS\":\n",
    "        optimizer = optim.LBFGS([random_image], lr=self.lr)\n",
    "        for step in progress_bar:\n",
    "            current_loss = [0.0]\n",
    "\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                style_loss = torch.zeros([], device=self.device, requires_grad=True)\n",
    "\n",
    "                style_outputs = self.model(random_image, self.style_layers_list)\n",
    "                for idx, o in enumerate(style_outputs):\n",
    "                    G = self.get_gram_matrix(o)\n",
    "                    style_loss = torch.add(\n",
    "                        style_loss,\n",
    "                        torch.mean((G - original_gram_matrices[idx]) ** 2)\n",
    "                        * self.style_weights[idx],\n",
    "                    )\n",
    "\n",
    "                style_loss.backward()\n",
    "\n",
    "                current_loss[0] = style_loss.item()\n",
    "                return style_loss\n",
    "\n",
    "            optimizer.step(closure)\n",
    "            progress_bar.set_postfix(loss=current_loss[0])\n",
    "\n",
    "    elif self._optim == \"Adam\":\n",
    "        optimizer = optim.Adam([random_image], lr=self.lr)\n",
    "        for step in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            style_loss = torch.zeros([], device=self.device, requires_grad=True)\n",
    "            style_outputs = self.model(random_image, self.style_layers_list)\n",
    "\n",
    "            for idx, o in enumerate(style_outputs):\n",
    "                G = self.get_gram_matrix(o)\n",
    "                style_loss = torch.add(\n",
    "                    self.style_weights[idx]\n",
    "                    * torch.mean((G - original_gram_matrices[idx]) ** 2),\n",
    "                    style_loss,\n",
    "                )\n",
    "\n",
    "            style_loss.backward()\n",
    "            optimizer.step()\n",
    "            progress_bar.set_postfix(loss=style_loss.item())\n",
    "    generated_image = self.tensor_to_image(random_image)\n",
    "    plt.imshow(generated_image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    return generated_image\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "---\n",
    "\n",
    "### Visualizing Content Representations at Different Layers:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td align=\"center\">\n",
    "      <img src=\"./Content_Visualization/relu1_1.png\" alt=\"Relu1_1\" width=\"100%\">\n",
    "      <br>Representation from Layer Relu1_1\n",
    "    </td>\n",
    "    <td align=\"center\">\n",
    "      <img src=\"./Content_Visualization/relu2_1.png\" alt=\"Relu2_1\" width=\"100%\">\n",
    "      <br>Representation from Layer Relu2_1\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td align=\"center\">\n",
    "      <img src=\"./Content_Visualization/relu3_1.png\" alt=\"Relu3_1\" width=\"100%\">\n",
    "      <br>Representation from Layer Relu3_1\n",
    "    </td>\n",
    "    <td align=\"center\">\n",
    "      <img src=\"./Content_Visualization/relu4_1.png\" alt=\"Relu4_1\" width=\"100%\">\n",
    "      <br>Representation from Layer Relu4_1\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td align=\"center\" colspan=\"2\">\n",
    "      <img src=\"./Content_Visualization/relu5_1.png\" alt=\"Relu5_1\" width=\"50%\">\n",
    "      <br>Representation from Layer Relu5_1\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "### Visualizing Style Representations at Different Layers:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td align=\"center\">\n",
    "      <img src=\"./Style_Visualization/relu1_1.png\" alt=\"Relu1_1\" width=\"100%\">\n",
    "      <br>Relu1_1\n",
    "    </td>\n",
    "    <td align=\"center\">\n",
    "      <img src=\"./Style_Visualization/relu1_1+relu2_1.png\" alt=\"Relu1_1 + Relu2_1\" width=\"100%\">\n",
    "      <br>Relu1_1 + Relu2_1\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td align=\"center\">\n",
    "      <img src=\"./Style_Visualization/relu1_1+relu2_1+relu3_1.png\" alt=\"Relu1_1 + Relu2_1 + Relu3_1\" width=\"100%\">\n",
    "      <br>Relu1_1 + Relu2_1 + Relu3_1\n",
    "    </td>\n",
    "    <td align=\"center\">\n",
    "      <img src=\"./Style_Visualization/relu1_1+relu2_1+relu3_1+relu4_1.png\" alt=\"Relu1_1 + Relu2_1 + Relu3_1 + Relu4_1\" width=\"100%\">\n",
    "      <br>Relu1_1 + Relu2_1 + Relu3_1 + Relu4_1\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td align=\"center\" colspan=\"2\">\n",
    "      <img src=\"./Style_Visualization/relu1_1+relu2_1+relu3_1+relu4_1+relu5_1.png\" alt=\"Relu1_1 + Relu2_1 + Relu3_1 + Relu4_1 + Relu5_1\" width=\"50%\">\n",
    "      <br>Relu1_1 + Relu2_1 + Relu3_1 + Relu4_1 + Relu5_1\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
